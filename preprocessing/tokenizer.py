from tensorflow.keras.preprocessing.text import Tokenizer

def create_tokenizers(df):
    # Tokenize English and Korean sentences
    pass